"""
Semantic Analyzer - Advanced semantic validation for AI-generated code.

Performs deep semantic analysis including:
- Code structure validation
- Logic flow analysis  
- Dependency validation
- API usage verification
- Best practice compliance
"""

import ast
import logging
import re
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Union, Tuple
from dataclasses import dataclass
from enum import Enum

from claude_tui.core.config_manager import ConfigManager
from claude_tui.models.project import Project
from claude_tui.models.task import DevelopmentTask
from claude_tui.validation.progress_validator import ValidationIssue, ValidationSeverity

logger = logging.getLogger(__name__)


class SemanticIssueType(Enum):
    """Types of semantic issues."""
    SYNTAX_ERROR = "syntax_error"
    LOGICAL_ERROR = "logical_error"
    UNUSED_IMPORT = "unused_import"
    UNDEFINED_VARIABLE = "undefined_variable"
    UNREACHABLE_CODE = "unreachable_code"
    CIRCULAR_IMPORT = "circular_import"
    INVALID_API_USAGE = "invalid_api_usage"
    MISSING_DEPENDENCY = "missing_dependency"
    PERFORMANCE_ISSUE = "performance_issue"
    SECURITY_ISSUE = "security_issue"
    STYLE_VIOLATION = "style_violation"


@dataclass
class SemanticContext:
    """Context for semantic analysis."""
    file_path: Optional[Path]
    language: str
    project: Optional[Project]
    imported_modules: Set[str]
    defined_variables: Set[str]
    defined_functions: Set[str]
    defined_classes: Set[str]
    dependencies: List[str]


@dataclass
class CodeElement:
    """Represents a code element for analysis."""
    name: str
    element_type: str  # 'function', 'class', 'variable', etc.
    start_line: int
    end_line: int
    complexity: int
    dependencies: List[str]
    used: bool = False


class SemanticAnalyzer:
    """
    Advanced semantic validation for AI-generated code.
    
    Performs comprehensive semantic analysis to detect logical errors,
    API misuse, dependency issues, and code quality problems.
    """
    
    def __init__(self, config_manager: ConfigManager):
        """
        Initialize the semantic analyzer.
        
        Args:
            config_manager: Configuration management instance
        """
        self.config_manager = config_manager
        
        # Language-specific analyzers
        self._python_analyzer = PythonSemanticAnalyzer()
        self._javascript_analyzer = JavaScriptSemanticAnalyzer()
        
        # Configuration
        self._strict_mode = True
        self._check_unused_imports = True
        self._check_performance = True
        self._check_security = True
        
        logger.info("Semantic analyzer initialized")
    
    async def initialize(self) -> None:
        """
        Initialize the semantic analyzer.
        """
        logger.info("Initializing semantic analyzer")
        
        try:
            # Load configuration
            analyzer_config = await self.config_manager.get_setting('semantic_analyzer', {})
            self._strict_mode = analyzer_config.get('strict_mode', True)
            self._check_unused_imports = analyzer_config.get('check_unused_imports', True)
            self._check_performance = analyzer_config.get('check_performance', True)
            self._check_security = analyzer_config.get('check_security', True)
            
            # Initialize language-specific analyzers
            await self._python_analyzer.initialize(analyzer_config)
            await self._javascript_analyzer.initialize(analyzer_config)
            
            logger.info("Semantic analyzer initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize semantic analyzer: {e}")
            raise
    
    async def analyze_content(\n        self,\n        content: str,\n        file_path: Optional[Path] = None,\n        project: Optional[Project] = None,\n        language: Optional[str] = None\n    ) -> List[ValidationIssue]:\n        \"\"\"\n        Analyze content for semantic issues.\n        \n        Args:\n            content: Content to analyze\n            file_path: File path for context\n            project: Associated project\n            language: Programming language (inferred if None)\n            \n        Returns:\n            List of validation issues\n        \"\"\"\n        if not content.strip():\n            return []\n        \n        # Infer language if not provided\n        if not language and file_path:\n            language = self._infer_language(file_path)\n        \n        if not language:\n            logger.warning(\"Could not determine language, skipping semantic analysis\")\n            return []\n        \n        logger.debug(f\"Analyzing semantic content for {language}\")\n        \n        try:\n            # Build semantic context\n            context = await self._build_semantic_context(\n                content, file_path, project, language\n            )\n            \n            issues = []\n            \n            # Language-specific analysis\n            if language == 'python':\n                lang_issues = await self._python_analyzer.analyze(\n                    content, context, file_path\n                )\n                issues.extend(lang_issues)\n            \n            elif language in ['javascript', 'typescript']:\n                lang_issues = await self._javascript_analyzer.analyze(\n                    content, context, file_path\n                )\n                issues.extend(lang_issues)\n            \n            # General semantic analysis\n            general_issues = await self._general_semantic_analysis(\n                content, context, language\n            )\n            issues.extend(general_issues)\n            \n            # Cross-reference analysis\n            if project:\n                cross_ref_issues = await self._cross_reference_analysis(\n                    content, context, project\n                )\n                issues.extend(cross_ref_issues)\n            \n            logger.debug(f\"Found {len(issues)} semantic issues\")\n            return issues\n            \n        except Exception as e:\n            logger.error(f\"Semantic analysis failed: {e}\")\n            return [ValidationIssue(\n                id=\"semantic_analysis_error\",\n                description=f\"Semantic analysis failed: {e}\",\n                severity=ValidationSeverity.MEDIUM,\n                file_path=str(file_path) if file_path else None,\n                issue_type=\"analysis_error\"\n            )]\n    \n    async def analyze_generated_content(\n        self,\n        content: str,\n        task: Optional[DevelopmentTask] = None,\n        project: Optional[Project] = None\n    ) -> List[ValidationIssue]:\n        \"\"\"\n        Analyze AI-generated content with task context.\n        \n        Args:\n            content: Generated content to analyze\n            task: Original task context\n            project: Associated project\n            \n        Returns:\n            List of validation issues\n        \"\"\"\n        issues = await self.analyze_content(\n            content=content,\n            project=project\n        )\n        \n        # Add task-specific validation\n        if task:\n            task_issues = await self._validate_task_semantic_requirements(\n                content, task, project\n            )\n            issues.extend(task_issues)\n        \n        return issues\n    \n    async def cleanup(self) -> None:\n        \"\"\"\n        Cleanup semantic analyzer resources.\n        \"\"\"\n        logger.info(\"Cleaning up semantic analyzer\")\n        \n        await self._python_analyzer.cleanup()\n        await self._javascript_analyzer.cleanup()\n        \n        logger.info(\"Semantic analyzer cleanup completed\")\n    \n    # Private helper methods\n    \n    def _infer_language(self, file_path: Path) -> Optional[str]:\n        \"\"\"\n        Infer programming language from file extension.\n        \"\"\"\n        extension_map = {\n            '.py': 'python',\n            '.js': 'javascript',\n            '.ts': 'typescript',\n            '.jsx': 'javascript',\n            '.tsx': 'typescript',\n            '.java': 'java',\n            '.cpp': 'cpp',\n            '.c': 'c',\n            '.cs': 'csharp',\n            '.go': 'go',\n            '.rs': 'rust',\n            '.php': 'php',\n            '.rb': 'ruby'\n        }\n        \n        return extension_map.get(file_path.suffix.lower())\n    \n    async def _build_semantic_context(\n        self,\n        content: str,\n        file_path: Optional[Path],\n        project: Optional[Project],\n        language: str\n    ) -> SemanticContext:\n        \"\"\"\n        Build semantic context for analysis.\n        \"\"\"\n        context = SemanticContext(\n            file_path=file_path,\n            language=language,\n            project=project,\n            imported_modules=set(),\n            defined_variables=set(),\n            defined_functions=set(),\n            defined_classes=set(),\n            dependencies=[]\n        )\n        \n        # Extract imports and definitions based on language\n        if language == 'python':\n            await self._extract_python_context(content, context)\n        elif language in ['javascript', 'typescript']:\n            await self._extract_javascript_context(content, context)\n        \n        return context\n    \n    async def _extract_python_context(self, content: str, context: SemanticContext) -> None:\n        \"\"\"\n        Extract Python-specific context information.\n        \"\"\"\n        try:\n            # Parse AST for accurate analysis\n            tree = ast.parse(content)\n            \n            for node in ast.walk(tree):\n                if isinstance(node, ast.Import):\n                    for alias in node.names:\n                        context.imported_modules.add(alias.name)\n                elif isinstance(node, ast.ImportFrom):\n                    if node.module:\n                        context.imported_modules.add(node.module)\n                elif isinstance(node, ast.FunctionDef):\n                    context.defined_functions.add(node.name)\n                elif isinstance(node, ast.ClassDef):\n                    context.defined_classes.add(node.name)\n                elif isinstance(node, ast.Name) and isinstance(node.ctx, ast.Store):\n                    context.defined_variables.add(node.id)\n        \n        except SyntaxError as e:\n            logger.warning(f\"Python syntax error during context extraction: {e}\")\n        except Exception as e:\n            logger.warning(f\"Failed to extract Python context: {e}\")\n    \n    async def _extract_javascript_context(self, content: str, context: SemanticContext) -> None:\n        \"\"\"\n        Extract JavaScript-specific context information.\n        \"\"\"\n        # Simple regex-based extraction for JavaScript\n        # In a real implementation, would use a proper JS parser\n        \n        # Extract imports\n        import_patterns = [\n            r'import\\s+.*?\\s+from\\s+[\"\\']([^\"\\'\n]+)[\"\\']',\n            r'require\\s*\\(\\s*[\"\\']([^\"\\'\n]+)[\"\\']\\s*\\)'\n        ]\n        \n        for pattern in import_patterns:\n            matches = re.findall(pattern, content)\n            context.imported_modules.update(matches)\n        \n        # Extract function definitions\n        function_patterns = [\n            r'function\\s+(\\w+)\\s*\\(',\n            r'(\\w+)\\s*=\\s*function\\s*\\(',\n            r'(\\w+)\\s*=\\s*\\([^)]*\\)\\s*=>',\n            r'const\\s+(\\w+)\\s*=\\s*\\([^)]*\\)\\s*=>'\n        ]\n        \n        for pattern in function_patterns:\n            matches = re.findall(pattern, content)\n            context.defined_functions.update(matches)\n        \n        # Extract class definitions\n        class_matches = re.findall(r'class\\s+(\\w+)', content)\n        context.defined_classes.update(class_matches)\n    \n    async def _general_semantic_analysis(\n        self,\n        content: str,\n        context: SemanticContext,\n        language: str\n    ) -> List[ValidationIssue]:\n        \"\"\"\n        General semantic analysis applicable to all languages.\n        \"\"\"\n        issues = []\n        \n        # Check for unused imports\n        if self._check_unused_imports:\n            unused_issues = await self._check_unused_imports_analysis(\n                content, context\n            )\n            issues.extend(unused_issues)\n        \n        # Check for basic logical issues\n        logical_issues = await self._check_logical_issues(\n            content, context\n        )\n        issues.extend(logical_issues)\n        \n        # Security checks\n        if self._check_security:\n            security_issues = await self._check_security_issues(\n                content, context\n            )\n            issues.extend(security_issues)\n        \n        # Performance checks\n        if self._check_performance:\n            performance_issues = await self._check_performance_issues(\n                content, context\n            )\n            issues.extend(performance_issues)\n        \n        return issues\n    \n    async def _check_unused_imports_analysis(\n        self,\n        content: str,\n        context: SemanticContext\n    ) -> List[ValidationIssue]:\n        \"\"\"\n        Check for unused imports.\n        \"\"\"\n        issues = []\n        \n        for imported_module in context.imported_modules:\n            # Simple check - look for module usage in content\n            module_name = imported_module.split('.')[-1]  # Get last part of module path\n            \n            # Check if module is used anywhere in the content\n            if not re.search(rf'\\b{re.escape(module_name)}\\b', content):\n                issues.append(ValidationIssue(\n                    id=f\"unused_import_{module_name}\",\n                    description=f\"Unused import: {imported_module}\",\n                    severity=ValidationSeverity.LOW,\n                    file_path=str(context.file_path) if context.file_path else None,\n                    issue_type=\"unused_import\",\n                    auto_fixable=True,\n                    suggested_fix=f\"Remove unused import: {imported_module}\"\n                ))\n        \n        return issues\n    \n    async def _check_logical_issues(\n        self,\n        content: str,\n        context: SemanticContext\n    ) -> List[ValidationIssue]:\n        \"\"\"\n        Check for logical issues in code.\n        \"\"\"\n        issues = []\n        \n        lines = content.split('\\n')\n        \n        for i, line in enumerate(lines, 1):\n            stripped = line.strip()\n            \n            # Check for unreachable code after return\n            if 'return ' in stripped and i < len(lines):\n                next_line = lines[i].strip() if i < len(lines) else \"\"\n                if next_line and not next_line.startswith('#') and not next_line.startswith('//'):\n                    # Check if it's not another return or end of function\n                    if not any(keyword in next_line for keyword in ['def ', 'function ', 'class ', '}']):\n                        issues.append(ValidationIssue(\n                            id=f\"unreachable_code_{i+1}\",\n                            description=\"Unreachable code after return statement\",\n                            severity=ValidationSeverity.MEDIUM,\n                            file_path=str(context.file_path) if context.file_path else None,\n                            line_number=i+1,\n                            issue_type=\"unreachable_code\",\n                            suggested_fix=\"Remove unreachable code or restructure logic\"\n                        ))\n            \n            # Check for potential infinite loops\n            if 'while True:' in stripped or 'for(;;)' in stripped:\n                # Look for break or return in the loop\n                loop_has_exit = False\n                for j in range(i, min(i+10, len(lines))):\n                    if 'break' in lines[j] or 'return' in lines[j]:\n                        loop_has_exit = True\n                        break\n                \n                if not loop_has_exit:\n                    issues.append(ValidationIssue(\n                        id=f\"potential_infinite_loop_{i}\",\n                        description=\"Potential infinite loop without exit condition\",\n                        severity=ValidationSeverity.HIGH,\n                        file_path=str(context.file_path) if context.file_path else None,\n                        line_number=i,\n                        issue_type=\"logical_error\",\n                        suggested_fix=\"Add break condition or return statement\"\n                    ))\n        \n        return issues\n    \n    async def _check_security_issues(\n        self,\n        content: str,\n        context: SemanticContext\n    ) -> List[ValidationIssue]:\n        \"\"\"\n        Check for security issues in code.\n        \"\"\"\n        issues = []\n        \n        # Common security anti-patterns\n        security_patterns = [\n            (r'eval\\s*\\(', \"Use of eval() function is dangerous\", ValidationSeverity.HIGH),\n            (r'exec\\s*\\(', \"Use of exec() function is dangerous\", ValidationSeverity.HIGH),\n            (r'innerHTML\\s*=', \"Direct innerHTML assignment can lead to XSS\", ValidationSeverity.MEDIUM),\n            (r'password\\s*=\\s*[\"\\'][^\"\\'\n]+[\"\\']', \"Hardcoded password detected\", ValidationSeverity.CRITICAL),\n            (r'api_key\\s*=\\s*[\"\\'][^\"\\'\n]+[\"\\']', \"Hardcoded API key detected\", ValidationSeverity.CRITICAL),\n            (r'subprocess\\.call\\s*\\(.*shell\\s*=\\s*True', \"Shell=True in subprocess can be dangerous\", ValidationSeverity.MEDIUM)\n        ]\n        \n        for pattern, description, severity in security_patterns:\n            matches = list(re.finditer(pattern, content, re.IGNORECASE))\n            for match in matches:\n                line_number = content[:match.start()].count('\\n') + 1\n                \n                issues.append(ValidationIssue(\n                    id=f\"security_issue_{line_number}_{hash(pattern)}\",\n                    description=description,\n                    severity=severity,\n                    file_path=str(context.file_path) if context.file_path else None,\n                    line_number=line_number,\n                    issue_type=\"security_issue\",\n                    suggested_fix=f\"Review and secure the usage: {match.group(0)[:30]}\"\n                ))\n        \n        return issues\n    \n    async def _check_performance_issues(\n        self,\n        content: str,\n        context: SemanticContext\n    ) -> List[ValidationIssue]:\n        \"\"\"\n        Check for performance issues in code.\n        \"\"\"\n        issues = []\n        \n        # Performance anti-patterns\n        performance_patterns = [\n            (r'for\\s+\\w+\\s+in\\s+range\\s*\\(\\s*len\\s*\\([^)]+\\)\\s*\\)', \"Use enumerate() instead of range(len())\", ValidationSeverity.LOW),\n            (r'\\.append\\s*\\([^)]+\\)\\s*(?:\\n\\s*)*\\}?\\s*(?:\\n\\s*)*\\}?\\s*(?:for|while)', \"List comprehension might be more efficient\", ValidationSeverity.LOW),\n            (r'\\+\\s*=.*\\+.*for\\s+', \"String concatenation in loop is inefficient\", ValidationSeverity.MEDIUM)\n        ]\n        \n        for pattern, description, severity in performance_patterns:\n            matches = list(re.finditer(pattern, content, re.IGNORECASE | re.DOTALL))\n            for match in matches:\n                line_number = content[:match.start()].count('\\n') + 1\n                \n                issues.append(ValidationIssue(\n                    id=f\"performance_issue_{line_number}_{hash(pattern)}\",\n                    description=description,\n                    severity=severity,\n                    file_path=str(context.file_path) if context.file_path else None,\n                    line_number=line_number,\n                    issue_type=\"performance_issue\",\n                    suggested_fix=f\"Consider optimization: {description}\"\n                ))\n        \n        return issues\n    \n    async def _cross_reference_analysis(\n        self,\n        content: str,\n        context: SemanticContext,\n        project: Project\n    ) -> List[ValidationIssue]:\n        \"\"\"\n        Cross-reference analysis with project context.\n        \"\"\"\n        issues = []\n        \n        # Check for undefined variables that might be defined in project\n        # This would require more sophisticated project analysis\n        \n        # Check for missing dependencies\n        for module in context.imported_modules:\n            # Simple check - this would be more sophisticated in reality\n            if module not in ['os', 'sys', 'json', 're', 'datetime', 'pathlib']:\n                # Check if it's a local import or external dependency\n                if '.' not in module and not module.startswith('.'):\n                    # Might be missing from requirements\n                    issues.append(ValidationIssue(\n                        id=f\"potential_missing_dep_{module}\",\n                        description=f\"Potentially missing dependency: {module}\",\n                        severity=ValidationSeverity.LOW,\n                        file_path=str(context.file_path) if context.file_path else None,\n                        issue_type=\"missing_dependency\",\n                        suggested_fix=f\"Ensure {module} is installed and in requirements\"\n                    ))\n        \n        return issues\n    \n    async def _validate_task_semantic_requirements(\n        self,\n        content: str,\n        task: DevelopmentTask,\n        project: Optional[Project]\n    ) -> List[ValidationIssue]:\n        \"\"\"\n        Validate semantic requirements specific to the task.\n        \"\"\"\n        issues = []\n        \n        # Check if generated content matches task requirements\n        if hasattr(task, 'semantic_requirements'):\n            for requirement in task.semantic_requirements:\n                if not self._check_semantic_requirement(content, requirement):\n                    issues.append(ValidationIssue(\n                        id=f\"task_semantic_requirement_{hash(requirement)}\",\n                        description=f\"Task semantic requirement not met: {requirement}\",\n                        severity=ValidationSeverity.HIGH,\n                        issue_type=\"task_requirement\",\n                        suggested_fix=f\"Implement semantic requirement: {requirement}\"\n                    ))\n        \n        return issues\n    \n    def _check_semantic_requirement(self, content: str, requirement: str) -> bool:\n        \"\"\"\n        Check if content meets a semantic requirement.\n        \"\"\"\n        # Simple implementation - would be more sophisticated in practice\n        requirement_lower = requirement.lower()\n        content_lower = content.lower()\n        \n        # Check for key terms in the requirement\n        requirement_words = requirement_lower.split()\n        return any(word in content_lower for word in requirement_words)\n\n\nclass PythonSemanticAnalyzer:\n    \"\"\"Python-specific semantic analyzer.\"\"\"\n    \n    async def initialize(self, config: Dict[str, Any]) -> None:\n        \"\"\"Initialize Python analyzer.\"\"\"\n        pass\n    \n    async def analyze(\n        self,\n        content: str,\n        context: SemanticContext,\n        file_path: Optional[Path]\n    ) -> List[ValidationIssue]:\n        \"\"\"Analyze Python content.\"\"\"\n        issues = []\n        \n        try:\n            # Parse AST for syntax validation\n            tree = ast.parse(content)\n            \n            # Analyze AST for Python-specific issues\n            issues.extend(await self._analyze_ast(tree, context, file_path))\n            \n        except SyntaxError as e:\n            issues.append(ValidationIssue(\n                id=\"python_syntax_error\",\n                description=f\"Python syntax error: {e.msg}\",\n                severity=ValidationSeverity.CRITICAL,\n                file_path=str(file_path) if file_path else None,\n                line_number=e.lineno,\n                column_number=e.offset,\n                issue_type=\"syntax_error\",\n                suggested_fix=\"Fix syntax error\"\n            ))\n        \n        return issues\n    \n    async def _analyze_ast(\n        self,\n        tree: ast.AST,\n        context: SemanticContext,\n        file_path: Optional[Path]\n    ) -> List[ValidationIssue]:\n        \"\"\"Analyze Python AST for issues.\"\"\"\n        issues = []\n        \n        # Walk AST and check for issues\n        for node in ast.walk(tree):\n            # Check for empty except blocks\n            if isinstance(node, ast.ExceptHandler):\n                if not node.body or (len(node.body) == 1 and isinstance(node.body[0], ast.Pass)):\n                    issues.append(ValidationIssue(\n                        id=f\"empty_except_{node.lineno}\",\n                        description=\"Empty except block\",\n                        severity=ValidationSeverity.MEDIUM,\n                        file_path=str(file_path) if file_path else None,\n                        line_number=node.lineno,\n                        issue_type=\"logical_error\",\n                        suggested_fix=\"Add proper exception handling\"\n                    ))\n        \n        return issues\n    \n    async def cleanup(self) -> None:\n        \"\"\"Cleanup Python analyzer.\"\"\"\n        pass\n\n\nclass JavaScriptSemanticAnalyzer:\n    \"\"\"JavaScript-specific semantic analyzer.\"\"\"\n    \n    async def initialize(self, config: Dict[str, Any]) -> None:\n        \"\"\"Initialize JavaScript analyzer.\"\"\"\n        pass\n    \n    async def analyze(\n        self,\n        content: str,\n        context: SemanticContext,\n        file_path: Optional[Path]\n    ) -> List[ValidationIssue]:\n        \"\"\"Analyze JavaScript content.\"\"\"\n        issues = []\n        \n        # JavaScript-specific checks would go here\n        # For now, return empty list\n        \n        return issues\n    \n    async def cleanup(self) -> None:\n        \"\"\"Cleanup JavaScript analyzer.\"\"\"\n        pass